{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "import pickle\n",
    "import scipy.stats\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from sklearn.metrics import mean_squared_error\n",
    "root = Path(\".\")\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUR decomposition is similar to SVD in a way that it also decomposes a given matrix into three different matrices C, U and R where R represents the diagonal matrix but it differs from SVD in the method through which C, U and R are formed\n",
    "</br> </br>\n",
    "Formally, a CUR matrix approximation of a matrix A is three matrices C, U, and R such that C is made from columns of A, R is made from rows of A, and that the product CUR closely approximates A. Usually the CUR is selected to be a rank-k approximation, which means that C contains k columns of A, R contains k rows of A, and U is a k-by-k matrix.\n",
    "</br> </br>\n",
    "Each row and column of A has some probability assigned to it which is equal to sum of squares of all entries in that row divided by sum of squares of all elements in the matrix. K rows and and K columns are sampled from the original matrix with respect to the probability that any row/column is picked proportional to sum of squares of entries it has.\n",
    "</br></br>\n",
    "Once the matrix is decomposed, the method of predicting the ratings is similar to SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUR:\n",
    "    def __init__(self, retained_energy) -> None:\n",
    "        self.read_dataset()\n",
    "        self.train_test_split()\n",
    "        self.generate_user_item_matrix()\n",
    "        self.U, self.S, self.V_T = self.CUR_decomp(500)\n",
    "        self.CUR_transform(retained_energy)\n",
    "        self.calculate_item_similarity()\n",
    "        \n",
    "        self.sim_top_k = [None for _ in range(self.item_cnt + 1)]\n",
    "        for i in range(1, self.item_cnt): \n",
    "            self.sim_top_k[i] = self.get_sim_items(i)\n",
    "\n",
    "    def CUR_decomp(self , red_dim): \n",
    "        C = np.zeros((self.matrix.shape[0], red_dim))\n",
    "        R = np.zeros((red_dim, self.matrix.shape[1]))\n",
    "        sum_of_squares = np.sum(self.matrix ** 2)\n",
    "        probability_colwise = np.sum(self.matrix ** 2, axis=0) / sum_of_squares\n",
    "        col_ids = np.arange(self.matrix.shape[1])\n",
    "\n",
    "        taken_cols = 0\n",
    "        taken_col_list = list()\n",
    "        dup_col_list = list()\n",
    "        \n",
    "        while(taken_cols < red_dim) : \n",
    "            cur_p = np.random.choice(col_ids, p = probability_colwise)\n",
    "            if cur_p not in taken_col_list : \n",
    "                C[:, taken_cols] = self.matrix[:, cur_p] / np.sqrt(probability_colwise[cur_p] * red_dim)\n",
    "                taken_cols += 1\n",
    "                taken_col_list.append(cur_p)\n",
    "                dup_col_list.append(1)\n",
    "            else : \n",
    "                get_id = taken_col_list.index(cur_p)\n",
    "                dup_col_list[get_id] += 1\n",
    "        \n",
    "        C = np.multiply(C, np.sqrt(dup_col_list))\n",
    "\n",
    "        sum_of_squares_row = np.sum(self.matrix ** 2, axis = 1)\n",
    "        probability_row_wise = sum_of_squares_row / sum_of_squares\n",
    "        col_ids = np.arange(self.matrix.shape[0])\n",
    "        taken_row = 0\n",
    "        taken_row_list = list()\n",
    "        dup_row_list = list()\n",
    "\n",
    "        while(taken_row < red_dim) : \n",
    "            cur_p = np.random.choice(col_ids, p = probability_row_wise)\n",
    "            if cur_p not in taken_row_list : \n",
    "                R[taken_row, :] = self.matrix[cur_p, :] / np.sqrt(probability_row_wise[cur_p] * red_dim)\n",
    "                taken_row += 1\n",
    "                taken_row_list.append(cur_p)\n",
    "                dup_row_list.append(1)\n",
    "            else : \n",
    "                get_id = taken_row_list.index(cur_p)\n",
    "                dup_row_list[get_id] += 1\n",
    "        \n",
    "        R = np.multiply(R.T, np.sqrt(dup_row_list)).T\n",
    "\n",
    "        Sigma = np.zeros((red_dim, red_dim))\n",
    "        for i, I in enumerate(taken_row_list): \n",
    "            for j, J in enumerate(taken_col_list):\n",
    "                Sigma[i, j] = self.matrix[I, J]\n",
    "        \n",
    "        cur_U, cur_S, cur_V_T = la.svd(Sigma, red_dim)\n",
    "        new_S = np.zeros((red_dim, red_dim))\n",
    "        for i in range(red_dim):\n",
    "            new_S[i, i] = cur_S[i]\n",
    "\n",
    "        cur_S = new_S \n",
    "        for cols in range(red_dim):\n",
    "            if cur_S[cols, cols] >= 1:\n",
    "                cur_S[cols, cols] = 1 / cur_S[cols, cols]\n",
    "            else: \n",
    "                cur_S[cols, cols] = 0\n",
    "        \n",
    "        cur_S_sq = np.dot(cur_S, cur_S)\n",
    "        U = np.dot(cur_V_T.T, np.dot(cur_S_sq, cur_U.T))\n",
    "        print(U.shape)\n",
    "        return C, U, R\n",
    "\n",
    "    def read_dataset(self):\n",
    "        self.rating_data = pd.read_csv('ml-1m/ratings.dat', header=None, sep='::')\n",
    "        self.rating_data.columns = ['UserID', 'ItemID', 'Rating', 'Timestamp']\n",
    "        self.rating_data.drop(columns=['Timestamp'], axis=1, inplace=True)\n",
    "        \n",
    "        self.item_cnt = max(self.rating_data['ItemID']) + 1\n",
    "        self.users_cnt = max(self.rating_data['UserID']) + 1\n",
    "\n",
    "    def train_test_split(self):\n",
    "        self.rating_train = self.rating_data.sample(frac=0.8, random_state=200)\n",
    "        self.rating_test = self.rating_data.drop(self.rating_train.index)\n",
    "\n",
    "    def generate_user_item_matrix(self):\n",
    "        self.matrix = np.zeros(shape=(self.users_cnt, self.item_cnt))\n",
    "        for row in self.rating_train.itertuples():\n",
    "            self.matrix[row.UserID][row.ItemID] = row.Rating\n",
    "\n",
    "    def get_dim(self, S, retained_energy): \n",
    "        sum_sq = 0\n",
    "        val_list = list()\n",
    "        for i in range(S.shape[0]) : \n",
    "            sum_sq += S[i, i] ** 2\n",
    "            val_list.append(S[i, i])\n",
    "        cur_sum, cur_d = 0, 0\n",
    "        for val in val_list:\n",
    "            if cur_sum >= sum_sq * retained_energy: \n",
    "                return cur_d \n",
    "            cur_sum += val ** 2\n",
    "            cur_d += 1\n",
    "        return cur_d\n",
    "\n",
    "    def similarity(self, A, B):\n",
    "        return (1.0 / (1.0 + la.norm(A - B)))\n",
    "\n",
    "    def CUR_transform(self, retained_energy): \n",
    "        red_dim = self.get_dim(self.S, retained_energy)\n",
    "\n",
    "        transformed_S = np.diag(self.S[:red_dim])\n",
    "        transformed_U = self.U[:, :red_dim]\n",
    "        transformed_V_T = self.V_T[:red_dim, :]\n",
    "        new_S = np.zeros((transformed_S.shape[0] , transformed_S.shape[0]))\n",
    "        print(new_S.shape)\n",
    "        for i in range(transformed_S.shape[0]) : \n",
    "            new_S[i, i] = transformed_S[i]\n",
    "        transformed_S = new_S\n",
    "        print(transformed_S.shape , transformed_U.shape , transformed_V_T.shape)\n",
    "        self.SVD_matrix = np.dot(transformed_U, transformed_S)\n",
    "        self.SVD_matrix = np.dot(self.SVD_matrix, transformed_V_T)\n",
    "    \n",
    "    def calculate_item_similarity(self):\n",
    "        self.item_similarity = np.zeros(shape=(self.item_cnt, self.item_cnt))\n",
    "        transformed_item_matrix = list()\n",
    "        for items in range(1, self.item_cnt): \n",
    "            transformed_item_matrix.append(self.SVD_matrix[items, :].T) \n",
    "\n",
    "        for item1 in range(1, self.item_cnt): \n",
    "            for item2 in range(item1 + 1, self.item_cnt): \n",
    "                sim = self.similarity(transformed_item_matrix[item1 - 1], transformed_item_matrix[item2 - 1])\n",
    "                \n",
    "                self.item_similarity[item1][item2] = self.item_similarity[item2][item1] = sim\n",
    "    \n",
    "    def get_sim_items(self, itemID): \n",
    "        sim_userID = list()\n",
    "        for item in range(1, self.item_similarity.shape[0]): \n",
    "            if item == itemID: \n",
    "                continue\n",
    "            sim_userID.append((self.item_similarity[itemID][item], item))\n",
    "        sim_userID.sort(key=lambda y: -y[0])\n",
    "        return sim_userID\n",
    "\n",
    "    def predict_SVD(self, userID, itemID, top_K):\n",
    "        similarity_sum = 0 \n",
    "        rating_sum, cnt = 0, 0\n",
    "        \n",
    "        for (item_s, items) in self.sim_top_k[itemID]:\n",
    "            if cnt == top_K: \n",
    "                break\n",
    "            if self.matrix[userID][items] == 0: \n",
    "                continue\n",
    "            similarity_sum += item_s\n",
    "            rating_sum += item_s * self.matrix[userID][items]\n",
    "            cnt += 1\n",
    "        if similarity_sum == 0:\n",
    "            return 0\n",
    "            \n",
    "        return (rating_sum / similarity_sum)\n",
    "    \n",
    "    def get_top_k(self, k=3):\n",
    "        start_prediction_time = datetime.now()\n",
    "        for users in range(1, self.users_cnt): \n",
    "            unrated_items = np.where(self.matrix[users] == 0)[0]\n",
    "            for items in unrated_items:\n",
    "                if items == 0:\n",
    "                    continue\n",
    "                assert(self.matrix[users][items] == 0)\n",
    "                self.matrix[users][items] = self.predict_SVD(users, items, k)\n",
    "        end_prediction_time = datetime.now()\n",
    "        print(\"Time for prediction\" , (end_prediction_time - start_prediction_time).total_seconds())\n",
    "\n",
    "    def get_rmse(self):\n",
    "        y_actual = list(self.rating_test.Rating)\n",
    "        y_pred = list()\n",
    "        for id, row in self.rating_test.iterrows(): \n",
    "            uid, itid = row['UserID'], row['ItemID']\n",
    "            y_pred.append(self.matrix[uid][itid])\n",
    "        return mean_squared_error(y_actual, y_pred) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\AppData\\Local\\Temp\\ipykernel_11768\\668386011.py:81: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.rating_data = pd.read_csv('ml-1m/ratings.dat', header=None, sep='::')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n",
      "(437, 437)\n",
      "(437, 437) (6041, 437) (437, 3953)\n",
      "Time for prediction 223.776183\n"
     ]
    }
   ],
   "source": [
    "cur_recom = CUR(0.9)\n",
    "cur_recom.get_top_k()\n",
    "cur_recom.get_rmse()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"CUR90\"\n",
    "dbfile = open(my_path, 'wb')     \n",
    "pickle.dump(cur_recom.matrix,dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1851374361914413\n"
     ]
    }
   ],
   "source": [
    "print(cur_recom.get_rmse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\AppData\\Local\\Temp\\ipykernel_11768\\668386011.py:81: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  self.rating_data = pd.read_csv('ml-1m/ratings.dat', header=None, sep='::')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n",
      "(500, 500)\n",
      "(500, 500) (6041, 500) (500, 3953)\n",
      "Time for prediction 220.19741\n"
     ]
    }
   ],
   "source": [
    "cur_recom = CUR(1.0)\n",
    "cur_recom.get_top_k()\n",
    "\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"CUR100\"\n",
    "dbfile = open(my_path, 'wb')     \n",
    "pickle.dump(cur_recom.matrix,dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1781062652249525\n"
     ]
    }
   ],
   "source": [
    "print(cur_recom.get_rmse())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crux-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
